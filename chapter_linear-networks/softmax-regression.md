# 软最大回归
:label:`sec_softmax`

在 :numref:`sec_linear_regression` 中，我们引入了线性回归，在 :numref:`sec_linear_scratch` 中从头开始完成实现，并再次使用 :numref:`sec_linear_concise` 中的深度学习框架的高级 API 来完成繁重的工作。

回归是我们达到的锤子，当我们想回答 * 多少？* 或 * 多少？* 问题。如果您想预测房屋将被售出的美元（价格）数量，或者棒球队可能获得的胜利数量，或者患者在出院前留院的天数，那么您可能正在寻找回归模型。

在实践中，我们更经常对 * 分类 * 感兴趣：不要问 “多少”，而是 “哪一个”：

* 此电子邮件是否属于垃圾邮件文件夹或收件箱？
* 此客户是否更有可能注册 * 或 * 不注册 * 订阅服务？
* 这图像是否描绘驴，狗，猫，还是公鸡？
* 下一部阿斯顿最有可能观看哪部电影？

在口语上，机器学习从业人员超载 * 分类 * 这个词来描述两个微妙不同的问题：(i) 我们只感兴趣的是将例子分配给类别（班级）的问题；和 (ii) 我们希望做软任务的问题，即评估每个类别都适用.区别往往变得模糊，部分原因是，即使我们只关心硬任务，我们仍然使用制作软任务的模型。

## 分类问题
:label:`subsec_classification-problem`

为了让我们的脚湿，让我们从一个简单的图像分类问题开始。这里，每个输入由一个 $2\times2$ 灰度图像组成。我们可以用一个标量表示每个像素值，给我们四个功能 $x_1, x_2, x_3, x_4$。此外，让我们假设每个图像属于 “猫”，“鸡” 和 “狗” 类别中的一个。

接下来，我们必须选择如何表示标签。我们有两个明显的选择。也许最自然的冲动是选择 $y \in \{1, 2, 3\}$，其中整数分别代表 $\{\text{dog}, \text{cat}, \text{chicken}\}$。这是在计算机上存储 * 此类信息的好方法。如果类别之间有一些自然的排序，比如说，如果我们试图预测 $\{\text{baby}, \text{toddler}, \text{adolescent}, \text{young adult}, \text{adult}, \text{geriatric}\}$，那么将这个问题转换为回归并保留这种格式可能是有意义的。

但是，一般的分类问题并不与班级之间的自然排序有关。幸运的是，统计人员很久以前发明了一种简单的方法来表示分类数据：* 一热编码 *。一个热编码是一个具有尽可能多的组件的向量，我们有类别。与特定实例类别对应的组件设置为 1，所有其他组件均设置为 0。在我们的例子中，标签 $y$ 将是一个三维矢量，其中 $(1, 0, 0)$ 对应于 “猫”，$(0, 1, 0)$ 对应于 “鸡”，$(0, 0, 1)$ 对应于 “狗”：

$$y \in \{(1, 0, 0), (0, 1, 0), (0, 0, 1)\}.$$

## 网络体系结构

为了估计与所有可能类相关的条件概率，我们需要一个具有多个输出的模型，每个类一个。为了解决线性模型的分类问题，我们需要尽可能多的仿射函数。每个输出将对应于其自己的仿射函数。在我们的例子中，由于我们有 4 个功能和 3 个可能的输出类别，我们需要 12 个标量来表示权重（$w$ 带下标），3 个标量来表示偏差（$b$ 带下标）。我们为每个输入计算这三个 * 日志 *、$o_1, o_2$ 和 $o_3$：

$$
\begin{aligned}
o_1 &= x_1 w_{11} + x_2 w_{12} + x_3 w_{13} + x_4 w_{14} + b_1,\\
o_2 &= x_1 w_{21} + x_2 w_{22} + x_3 w_{23} + x_4 w_{24} + b_2,\\
o_3 &= x_1 w_{31} + x_2 w_{32} + x_3 w_{33} + x_4 w_{34} + b_3.
\end{aligned}
$$

我们可以用 :numref:`fig_softmaxreg` 中显示的神经网络图来描述这个计算。与线性回归一样，softmax 回归也是一个单层神经网络。由于计算每个输出，$o_1, o_2$ 和 $o_3$，取决于所有输入，$x_1$，$x_2$，$x_3$ 和 $x_4$，所以 softmax 回归的输出层也可以描述为完全连接层。

![Softmax regression is a single-layer neural network.](../img/softmaxreg.svg)
:label:`fig_softmaxreg`

为了更紧凑地表达模型，我们可以使用线性代数表示法。在矢量形式中，我们到达 $\mathbf{o} = \mathbf{W} \mathbf{x} + \mathbf{b}$，这是一种更适合数学和编写代码的形式。请注意，我们已经将我们的所有权重收集到 $3 \times 4$ 矩阵中，对于给定数据点 $\mathbf{x}$ 的要素，我们的输出是由我们的权重的矩阵矢量积通过我们的输入要素加上我们的偏差 $\mathbf{b}$ 给出的。

## 软最大操作

我们在这里要采取的主要方法是将模型的输出解释为概率。我们将优化我们的参数以产生可能性最大化观测数据的概率。然后，为了生成预测，我们将设置一个阈值，样本，选择具有最大预测概率的标签。

正式地说，我们希望任何输出 $\hat{y}_j$ 被解释为给定项目属于类 $j$ 的概率。然后我们可以选择具有最大输出值的类作为我们的预测 $\operatorname*{argmax}_j y_j$。样本如，如果 $\hat{y}_1$、$\hat{y}_2$ 和 $\hat{y}_3$ 分别为 0.1、0.8 和 0.1，那么我们预测类别 2，其中（在我们的例子中）代表 “鸡”。

您可能会试图建议我们将对数 $o$ 直接解释为我们感兴趣的输出。但是，直接将线性图层的输出解释为概率时存在一些问题。一方面，没有什么限制这些数字的总和为 1。另一方面，根据输入，它们可以采取负值。这些违反了 :numref:`sec_prob` 中提出的概率的基本公理

要将我们的输出解释为概率，我们必须保证（即使在新数据中），它们将是非负的，总和高达 1。此外，我们需要一个培训目标，鼓励模型忠实地估计概率。在分类器输出 0.5 的所有实例中，我们希望这些示例中的一半实际上属于预测类。这是一个名为 * 校准 * 的属性。

* softmax 功能 *，由社会科学家 R. 邓肯·卢斯在 * 选择模型 * 的背景下发明于 1959 年，正是这样做的。为了将我们的对数变换为非负并且总和为 1，同时要求模型保持可分化，我们首先指数每个对数（确保非负数），然后除以它们的总和（确保它们总和为 1）：

$$\hat{\mathbf{y}} = \mathrm{softmax}(\mathbf{o})\quad \text{where}\quad \hat{y}_j = \frac{\exp(o_j)}{\sum_k \exp(o_k)}. $$
:eqlabel:`eq_softmax_y_and_o`

这是很容易看到的所有的 $j$ 和 $0 \leq \hat{y}_j \leq 1$。因此，$\hat{\mathbf{y}}$ 是一个正确的概率分布，其元素值可以相应地解释。请注意，softmax 运算不会改变对数 $\mathbf{o}$ 之间的顺序，这只是确定分配给每个类的概率的软最大前值。因此，在预测过程中，我们仍然可以类过

$$
\operatorname*{argmax}_j \hat y_j = \operatorname*{argmax}_j o_j.
$$

虽然 softmax 是一个非线性函数，但 softmax 回归的输出仍由输入要素的仿射变换 * 确定 *；因此，softmax 回归是一个线性模型。

## 微型批次的矢量化
:label:`subsec_softmax_vectorization`

为了提高计算效率并利用 GPU，我们通常会针对小批量数据执行矢量计算。假设我们得到了一个小批次 $\mathbf{X}$ 的示例，其中包含特征维度（输入数量）$d$ 和批量大小 $n$。此外，假设我们在输出中有 $q$ 个类别。然后，微型批次特征 $\mathbf{X}$ 在 $\mathbb{R}^{n \times d}$ 中，重量为 $\mathbf{W} \in \mathbb{R}^{d \times q}$，偏差满足 $\mathbf{b} \in \mathbb{R}^{1\times q}$。

$$ \begin{aligned} \mathbf{O} &= \mathbf{X} \mathbf{W} + \mathbf{b}, \\ \hat{\mathbf{Y}} & = \mathrm{softmax}(\mathbf{O}). \end{aligned} $$
:eqlabel:`eq_minibatch_softmax_reg`

这加快了矩阵矩阵积 $\mathbf{X} \mathbf{W}$ 的主要运算，而我们一次处理一个样本时将要执行的矩阵矢量产品。由于 $\mathbf{X}$ 中的每一行代表一个数据点，因此软件最大运算本身可以计算 * rowwise*：对于 $\mathbf{O}$ 的每一行，对所有条目进行指数化，然后按总和对它们进行规范化。在 :eqref:`eq_minibatch_softmax_reg` 的总和期间触发广播 $\mathbf{X} \mathbf{W} + \mathbf{b}$, 无论是小批量对数 $\mathbf{O}$ 和输出概率 $\hat{\mathbf{Y}}$ 是 $n \times q$ 矩阵.

## 损失函数

接下来，我们需要一个损失函数来测量预测概率的质量。我们将依赖最大似然估计，这与我们在线性回归中为均方误差目标 (:numref:`subsec_normal_distribution_and_squared_loss`) 提供概率理由时遇到的概率理由相同。

### 对数似然

软最大函数给了我们一个向量 $\hat{\mathbf{y}}$，我们可以将其解释为每个类的估计条件概率，给定任何输入 $\mathbf{x}$，例如，$\hat{y}_1$ = $P(y=\text{cat} \mid \mathbf{x})$。假设整个数据集 $\{\mathbf{X}, \mathbf{Y}\}$ 具有 $n$ 样本，其中由 $i$ 索引的示例由特征向量 $\mathbf{x}^{(i)}$ 和单热标签向量 $\mathbf{y}^{(i)}$ 组成。我们可以通过根据我们的模型检查实际类的可能性来比较估计值和现实，给出的特征：

$$
P(\mathbf{Y} \mid \mathbf{X}) = \prod_{i=1}^n P(\mathbf{y}^{(i)} \mid \mathbf{x}^{(i)}).
$$

根据最大似然估计，我们最大化 $P(\mathbf{Y} \mid \mathbf{X})$，相当于最小化负对数似然：

$$
-\log P(\mathbf{Y} \mid \mathbf{X}) = \sum_{i=1}^n -\log P(\mathbf{y}^{(i)} \mid \mathbf{x}^{(i)})
= \sum_{i=1}^n l(\mathbf{y}^{(i)}, \hat{\mathbf{y}}^{(i)}),
$$

其中，对于任何一对标签 $\mathbf{y}$ 和模型预测 $\hat{\mathbf{y}}$，损失函数为

$$ l(\mathbf{y}, \hat{\mathbf{y}}) = - \sum_{j=1}^q y_j \log \hat{y}_j. $$
:eqlabel:`eq_l_cross_entropy`

由于后面解释的原因，:eqref:`eq_l_cross_entropy` 中的损失函数通常称为 * 交叉熵损失 *。由于 $\mathbf{y}$ 是一个长度为 $q$ 的单热向量，所以其所有坐标 $j$ 上的总和都消失了，除了一个项以外的所有项。由于所有 $\hat{y}_j$ 都是预测的概率，因此它们的对数永远不会大于 $0$。因此，如果我们正确地预测具有 * 确定性 * 的实际标签，即，如果实际标签 $\mathbf{y}$ 的预测概率 $P(\mathbf{y} \mid \mathbf{x}) = 1$，则损失函数无法进一步最小化。请注意，这往往是不可能的。例如，数据集中可能存在标签杂色（某些样本可能被误标）。当输入要素信息不够充分，无法对每个样本进行完美分类时，也可能无法实现。

### 软最大和衍生品
:label:`subsec_softmax_and_derivatives`

由于 softmax 和相应的损失是如此常见的，因此值得更好地理解它的计算方式。将 :eqref:`eq_softmax_y_and_o` 插入 :eqref:`eq_l_cross_entropy` 中的损失定义，并使用我们获得的软最大值的定义：

$$
\begin{aligned}
l(\mathbf{y}, \hat{\mathbf{y}}) &=  - \sum_{j=1}^q y_j \log \frac{\exp(o_j)}{\sum_{k=1}^q \exp(o_k)} \\
&= \sum_{j=1}^q y_j \log \sum_{k=1}^q \exp(o_k) - \sum_{j=1}^q y_j o_j\\
&= \log \sum_{k=1}^q \exp(o_k) - \sum_{j=1}^q y_j o_j.
\end{aligned}
$$

为了更好地理解发生了什么，考虑相对于任何逻辑 $o_j$ 的衍生物。我们得到

$$
\partial_{o_j} l(\mathbf{y}, \hat{\mathbf{y}}) = \frac{\exp(o_j)}{\sum_{k=1}^q \exp(o_k)} - y_j = \mathrm{softmax}(\mathbf{o})_j - y_j.
$$

换句话说，导数是我们模型分配的概率（由 softmax 运算表示）与实际发生的情况之间的差异，如单热标签向量中的元素所表示。在这个意义上，它与我们在回归中看到的非常相似，其中梯度是观测值 $y$ 和估计值 $\hat{y}$ 之间的差异。这不是巧合。在任何指数族（参见 [online appendix on distributions](https://d2l.ai/chapter_appendix-mathematics-for-deep-learning/distributions.html)）模型中，对数似然的梯度都是通过此项精确给出的。这一事实使得计算梯度在实践中变得很容易。

### 交叉熵损耗

现在考虑一下这样的情况：我们不仅观察到一个单一的结果，而且观察到结果的整个分布。对于标签 $\mathbf{y}$，我们可以使用与以前相同的表示形式。唯一的区别是，我们现在有一个通用概率向量，比如说 $(0, 0, 1)$，而不是仅包含二进制条目的向量，比如 $(0.1, 0.2, 0.7)$。我们之前用来定义损失 $l$ 在 :eqref:`eq_l_cross_entropy` 的数学仍然工作得很好, 只是解释稍微更一般.它是标签上分布的预期损失值。此损失称为 * 交叉熵损失 *，它是分类问题最常用的损失之一。我们可以通过介绍信息理论的基础知识来解释这个名字。如果你想了解更多信息理论的细节，你可以进一步参考 [online appendix on information theory](https://d2l.ai/chapter_appendix-mathematics-for-deep-learning/information-theory.html)。

## 信息理论基础

*信息理论 * 涉及编码, 解码, 发送,
以及尽可能简洁的形式处理信息 (也称为数据).

### 熵

信息理论的核心思想是量化数据中的信息内容。此数量对我们压缩数据的能力造成了严格限制。在信息理论中，这个数量被称为分布 $p$ 的 * 熵 *，并通过以下方程捕获：

$$H[p] = \sum_j - p(j) \log p(j).$$
:eqlabel:`eq_softmax_reg_entropy`

信息理论的基本定理之一指出，为了对从分布 $p$ 中随机抽取的数据进行编码，我们至少需要 $H[p]$ “nats” 对其进行编码。如果你想知道什么是 “nat”，它相当于位，但是当使用基础 $e$ 而不是基础 2 的代码时。因此，一个纳特是 $\frac{1}{\log(2)} \approx 1.44$ 位。

### 惊喜

你可能想知道压缩与预测有什么关系。想象一下，我们有一个要压缩的数据流。如果我们总是很容易预测下一个令牌，那么这个数据很容易压缩！举一个极端的样本，流中的每个令牌总是采用相同的值。这是一个非常无聊的数据流！而且不仅是无聊的，但它也很容易预测。因为它们总是相同的，所以我们不必传输任何信息来传递流的内容。易于预测，易于压缩。

但是，如果我们不能完全预测每一个事件，那么我们有时可能会感到惊讶。当我们分配一个事件的概率较低时，我们的惊喜更大。克劳德·香农定居于 $\log \frac{1}{P(j)} = -\log P(j)$，以量化一个人的 * 惊喜 *，在观察一个事件 $j$，并赋予它（主观）概率 $P(j)$。:eqref:`eq_softmax_reg_entropy` 中定义的熵是 * 预期的惊喜 *，当一个人分配了真正匹配数据生成过程的正确概率。

### 重新审视交叉熵

所以，如果熵是知道真正概率的人经历的惊喜水平，那么你可能会想知道，什么是交叉熵？交叉熵 * 从 * $p$ * 到 * $q$，表示 $H(p, q)$，是主观概率 $q$ 的观察者在看到根据概率 $p$ 实际生成的数据时的预期惊喜。$p=q$ 时实现了最低可能的交叉熵。在这种情况下，从 $p$ 到 $q$ 的交叉熵是 $H(p, p)= H(p)$。

简而言之，我们可以通过两种方式来考虑交叉熵分类目标：(i) 最大限度地提高观测数据的可能性；(ii) 最大限度地减少传达标签所需的惊喜（从而减少位数）。

## 模型预测和评估

在训练 softmax 回归模型后，给出任何样本特征，我们可以预测每个输出类的概率。通常，我们使用预测概率最高的类作为输出类。如果预测与实际类（标签）一致，则预测是正确的。在实验的下一部分，我们将使用 * 准确性 * 来评估模型的性能。这等于正确预测数与预测总数之间的比率。

## 摘要

* softmax 运算采用一个向量并将其映射到概率。
* Softmax 回归适用于分类问题。它在 softmax 运算中使用输出类的概率分布。
* 交叉熵是衡量两个概率分布之间差异的一个很好的度量。它测量给定模型的数据编码所需的位数。

## 练习

1. 我们可以更深入地探讨指数系列与 softmax 之间的联系。
    * 计算软最大交叉熵损耗 $l(\mathbf{y},\hat{\mathbf{y}})$ 的第二个导数。
    * 计算 $\mathrm{softmax}(\mathbf{o})$ 给出的分布的方差，并显示它与上面计算的第二个导数匹配。
1. 假设我们有三个类发生的概率相等，即概率向量是 $(\frac{1}{3}, \frac{1}{3}, \frac{1}{3})$。
    * 如果我们尝试为它设计二进制代码，有什么问题？
    * 你能设计一个更好的代码吗？提示：如果我们尝试编码两个独立的观察结果会发生什么？如果我们联合编码 $n$ 观测值怎么办？
1. Softmax 是上面介绍的映射的错误用语（但深度学习的每个人都使用它）。真正的软最大值被定义为 $\mathrm{RealSoftMax}(a, b) = \log (\exp(a) + \exp(b))$。
    * 证明这一点。
    * 证明这一点持有 $\lambda^{-1} \mathrm{RealSoftMax}(\lambda a, \lambda b)$，前提是 $\lambda > 0$。
    * 显示我们对于 $\lambda \to \infty$ 而言，我们有 $\lambda^{-1} \mathrm{RealSoftMax}(\lambda a, \lambda b) \to \mathrm{max}(a, b)$。
    * 软分钟是什么样子？
    * 将其扩展到两个以上的数字。

[Discussions](https://discuss.d2l.ai/t/46)
