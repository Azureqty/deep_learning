# 来自Transformer的双向编码器表示（BERT）

在“词嵌入”章节中，我们提到了词向量是用来表示词的向量，也就是说词向量是能够反映出语义的特征。但常用的词嵌入模型在训练完成后，每个词向量就固定了。在之后使用的时候，无论出现这个词的上下文是什么，词向量都不会随着上下文发生变化。例如“apple”这个词既有水果的意思，又是一家公司的名字，这种词向量叫做静态词向量。而我们期待一个好的词向量应该能够随着不同上下文产生变化，这种能够随着上下文语境不同而变化的词向量叫做动态词向量。

既然需要随着不同的上下文产生变化，我们可以设计一种动态计算词向量的网络，这个网络的输入是每个词的静态词向量，输出是每个词在当前上下文中的词向量。而这个网络也类似于词嵌入模型，可以预先在大量的预料中进行训练。这种网络叫做语言表示模型。

来自Transformer的双向编码器表示（BERT）[21]就是这样一种语言表示模型，它旨在通过训练上下文来预训练深度双向表示。因此，只需要一个额外的输出层，就可以对预训练的 BERT 表示进行微调，从而适用于广泛的任务，而无需对特定于任务进行大量模型结构的修改。

## 模型结构

BERT的基础模型结构是在“Transformer”章节中描述的多层双向Transformer编码器。原始的Transformer包括编码器和解码器部分。由于BERT的目标是语言表示模型，因此只需要编码器机制。

### 输入表示

BERT的输入支持单个句子或一对句子。分别适用于单句任务（如文本分类任务）和句对任务（如自然语言推理任务）。BERT的输入包含三部分，分别是令牌嵌入、片段嵌入、位置嵌入。

令牌嵌入（Token Embeddings）是将各个词转换成固定维度的向量。首先在序列的开始位置加入特殊标记“[CLS]”，在每个句子序列的结束位置都要加入“[SEP]”。如果有两个句子，直接拼接在一起，也在每个句子序列的结束位置加入“[SEP]”。在BERT中，每个词会被转换成768维的向量表示。

片段嵌入（Segment Embeddings）是为了使BERT能够处理句对的输入，句子对中的两个句子被简单的拼接在一起作为输入，因为我们需要使模型能够区分一个句子对中的两个句子，这就是片段嵌入的作用。片段嵌入只有两种向量表示，把向量0给第一个句子序列中的每个令牌，把向量1给第二个句子序列中的每个令牌。如果是输入仅仅有一个句子，那序列中的每个令牌的片段嵌入都是向量0。向量0和向量1都是在训练过程中更新得到的。每个向量都是768维度，所以片段嵌入层的大小是（2，768）。

位置嵌入（Position Embeddings）。为了解决Transformer无法编码序列的问题，我们引入了位置嵌入。在BERT中的位置嵌入与Transformer里的位置嵌入稍有不同，BERT中的位置嵌入是在各个位置上学习一个向量表示，从而来将顺序的信息编码进来。BERT最长能处理512个令牌的序列，所以位置嵌入层的大小是（512，768）。

对于一个长度为n的输入序列，我们将有令牌嵌入（n，768）用来表示词，片段嵌入（n，768）用来区分两个句子，位置嵌入（n，768）用来学习到顺序。将这三种嵌入按元素相加，得到一个（n，768）的表示，这一表示就是BERT的输入。

### 遮蔽语言模型（mask-lm）
一般来说语言表示模型只能从左到右或者从右到左的单向训练。因为如果允许双向训练就意味着会使得每个词在多层的网络中间接地“看到自己”。
为了训练深度双向的表示，BERT设计了一种完形填空的猜词任务，名为遮蔽语言模型的任务。具体来说，就是随机将一定比例的输入标记替换为遮蔽标记“[MASK]”，然后预测这些被遮蔽的标记。即将遮蔽标记对应的输入隐藏向量输入一个单层网络，用softmax计算词汇表中每个单词的概率，以预测遮蔽标记应该对应哪个词。在BERT的设计中，在每个序列中随机遮蔽 15% 的标记。
由于在预训练阶段，我们使用了“[MASK]”这个遮蔽标记，但“[MASK]”在微调阶段并不会出现。这会带来预训练和微调之间的不匹配。BERT采用了一些策略来缓解这一问题。在选择了需要被遮蔽的标记后，80%的概率使用“[MASK]”标记进行替换（如my dog is hairy → my dog is [MASK]），10%的概率使用随机单词来替换（如my dog is hairy → my dog is apple），10%的概率保持被选择的单词不变（my dog is hairy → my dog is hairy）。

### 下一句预测

在自然语言处理中有很多下游任务是建立在理解两个句子之间关系的基础上。比如自然语言推理任务。这并不是语言模型所能直接学习到的。为了能够学习到句子间关系，BERT设计了一个预测下一句的二分类任务，即预测输入的两个句子是否为连续的文本。具体就是为每个训练样本选择句子A和B时，50%的概率B是A真实的下一句，有一半的概率使用来自语料库的随机句子替换句子B。
例如：

> 输入：[CLS] the man went to [MASK] store [SEP]
> 			he bought a gallon [MASK] milk [SEP]
> 标签：IsNext

> 前提：[CLS] the man [MASK] to the store [SEP]
> 			penguin [MASK] are flight ##less birds [SEP]
> 标签：NotNext

然后将“[CLS]”标记的输出送入一个单层网络，并使用softmax计算IsNext标签的概率，以判断句子是否是当前句子的下一句。使用“[CLS]”是因为Transformer是可以把全局信息编码进每个位置，因此“[CLS]”位置的输出表示可以包含整个输入序列的特征。

## 下游任务

在获得训练好的BERT后，最终只需在BERT的输出层上加简单的多层感知机或线性分类器即可。

对于单句和句对分类任务，直接取“[CLS]”位置的输出表示作为下游任务的输入。
对于问答这种抽取式任务，取第二个句子每个位置的输出表示作为下游任务的输入。
对于序列标注任务，取除了“[CLS]”位置外其他位置的输出表示作为下游任务的输入。

## 小结

- 随着上下文语境不同而变化的词向量叫做动态词向量。
- BERT旨在通过训练上下文来预训练深度双向表示。
- BERT的基础模型结构是在“Transformer”章节中描述的多层双向Transformer编码器。
- 遮蔽语言模型是随机将输入标记遮蔽，然后预测这些被遮蔽的标记。
- 预测下一句的二分类任务，预测输入的两个句子是否为连续的文本。

## 参考文献

[1] Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.

## 扫码直达[讨论区](https://discuss.gluon.ai/t/topic/7762)
