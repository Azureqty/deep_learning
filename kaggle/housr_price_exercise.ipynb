{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 训练方法：\n",
    "[K-Fold Cross-Validation for Neural Networks](https://jamesmccaffrey.wordpress.com/2013/10/25/k-fold-cross-validation-for-neural-networks/)\n",
    "```\n",
    "loop \"many\" times\n",
    "  pick a number of hidden nodes\n",
    "  pick training parameters (learning rate, etc.)\n",
    "  \n",
    "  // k-fold\n",
    "  divide train data into 10 parts\n",
    "  for i = 1 to 10\n",
    "    train network using 9 parts\n",
    "    compute accuracy using 1 part\n",
    "  end for\n",
    "  compute average accuracy of the 10 runs\n",
    "\n",
    "  if avg accuracy best found so far\n",
    "    save number hidden nodes used\n",
    "    save training parameters used\n",
    "    save best average accuracy value\n",
    "  end if\n",
    "end loop\n",
    "\n",
    "train network using all data\n",
    "  (using best number hidden nodes,\n",
    "   and best training parameters)\n",
    "estimated accuracy is best accuracy found above\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-10-31T15:48:59.071Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train = pd.read_csv(\"../data/kaggle_house_pred_train.csv\")\n",
    "test = pd.read_csv(\"../data/kaggle_house_pred_test.csv\")\n",
    "all_X = pd.concat((train.loc[:, 'MSSubClass':'SaleCondition'],\n",
    "                      test.loc[:, 'MSSubClass':'SaleCondition']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-10-31T15:46:25.181Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numeric_feats = all_X.dtypes[all_X.dtypes != \"object\"].index\n",
    "all_X[numeric_feats] = all_X[numeric_feats].apply(lambda x: (x - x.mean())\n",
    "                                                            / (x.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-31T15:30:38.746077Z",
     "start_time": "2017-10-31T15:30:38.668253Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_X = pd.get_dummies(all_X, dummy_na=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-31T15:30:39.213377Z",
     "start_time": "2017-10-31T15:30:39.103847Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_X = all_X.fillna(all_X.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-31T15:30:39.620928Z",
     "start_time": "2017-10-31T15:30:39.600228Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_train = train.shape[0]\n",
    "\n",
    "X_train = all_X[:num_train].as_matrix()\n",
    "X_test = all_X[num_train:].as_matrix()\n",
    "y_train = train.SalePrice.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-31T15:30:40.102947Z",
     "start_time": "2017-10-31T15:30:40.078036Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mxnet import ndarray as nd\n",
    "from mxnet import autograd\n",
    "from mxnet import gluon\n",
    "\n",
    "X_train = nd.array(X_train)\n",
    "y_train = nd.array(y_train)\n",
    "y_train.reshape((num_train, 1))\n",
    "\n",
    "X_test = nd.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-31T15:30:40.670957Z",
     "start_time": "2017-10-31T15:30:40.666295Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "square_loss = gluon.loss.L2Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-31T15:30:41.081757Z",
     "start_time": "2017-10-31T15:30:41.076542Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_rmse_log(net, X_train, y_train):\n",
    "    num_train = X_train.shape[0]\n",
    "    clipped_preds = nd.clip(net(X_train), 1, float('inf'))\n",
    "    return np.sqrt(2 * nd.sum(square_loss(\n",
    "        nd.log(clipped_preds), nd.log(y_train))).asscalar() / num_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-31T15:30:41.459946Z",
     "start_time": "2017-10-31T15:30:41.456037Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "k = 5\n",
    "epochs = 100\n",
    "verbose_epoch = 95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-31T15:30:41.947472Z",
     "start_time": "2017-10-31T15:30:41.942100Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_net(hidden_nodes):\n",
    "    net = gluon.nn.Sequential()\n",
    "    with net.name_scope():\n",
    "        net.add(gluon.nn.Dense(hidden_nodes))\n",
    "        net.add(gluon.nn.Dense(1))\n",
    "    net.initialize()\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-31T15:30:43.263420Z",
     "start_time": "2017-10-31T15:30:43.203756Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.dpi']= 120\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train(net, X_train, y_train, X_test, y_test, epochs,\n",
    "          verbose_epoch, learning_rate, weight_decay):\n",
    "    train_loss = []\n",
    "    if X_test is not None:\n",
    "#         test_loss = []\n",
    "        pass\n",
    "    batch_size = 100\n",
    "    dataset_train = gluon.data.ArrayDataset(X_train, y_train)\n",
    "    data_iter_train = gluon.data.DataLoader(\n",
    "        dataset_train, batch_size,shuffle=True)\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'adam',\n",
    "                            {'learning_rate': learning_rate,\n",
    "                             'wd': weight_decay})\n",
    "    net.collect_params().initialize(force_reinit=True)\n",
    "    for epoch in range(epochs):\n",
    "        for data, label in data_iter_train:\n",
    "            with autograd.record():\n",
    "                output = net(data)\n",
    "                loss = square_loss(output, label)\n",
    "            loss.backward()\n",
    "            trainer.step(batch_size)\n",
    "\n",
    "            cur_train_loss = get_rmse_log(net, X_train, y_train)\n",
    "        if epoch > verbose_epoch:\n",
    "            print(\"Epoch %d, train loss: %f\" % (epoch, cur_train_loss))\n",
    "#         train_loss.append(cur_train_loss)\n",
    "        if X_test is not None:\n",
    "            cur_test_loss = get_rmse_log(net, X_test, y_test)\n",
    "#             test_loss.append(cur_test_loss)\n",
    "#     plt.plot(train_loss)\n",
    "#     plt.legend(['train'])\n",
    "#     if X_test is not None:\n",
    "#         plt.plot(test_loss)\n",
    "#         plt.legend(['train','test'])\n",
    "#     plt.show()\n",
    "    if X_test is not None:\n",
    "        return cur_train_loss, cur_test_loss\n",
    "    else:\n",
    "        return cur_train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-31T15:30:44.278772Z",
     "start_time": "2017-10-31T15:30:44.249421Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def k_fold_cross_valid(k, epochs, verbose_epoch, X_train, y_train,\n",
    "                       learning_rate, weight_decay, net):\n",
    "    assert k > 1\n",
    "    fold_size = X_train.shape[0] // k\n",
    "    train_loss_sum = 0.0\n",
    "    test_loss_sum = 0.0\n",
    "    for test_i in range(k):\n",
    "        X_val_test = X_train[test_i * fold_size: (test_i + 1) * fold_size, :]\n",
    "        y_val_test = y_train[test_i * fold_size: (test_i + 1) * fold_size]\n",
    "\n",
    "        val_train_defined = False\n",
    "        for i in range(k):\n",
    "            if i != test_i:\n",
    "                X_cur_fold = X_train[i * fold_size: (i + 1) * fold_size, :]\n",
    "                y_cur_fold = y_train[i * fold_size: (i + 1) * fold_size]\n",
    "                if not val_train_defined:\n",
    "                    X_val_train = X_cur_fold\n",
    "                    y_val_train = y_cur_fold\n",
    "                    val_train_defined = True\n",
    "                else:\n",
    "                    X_val_train = nd.concat(X_val_train, X_cur_fold, dim=0)\n",
    "                    y_val_train = nd.concat(y_val_train, y_cur_fold, dim=0)\n",
    "        train_loss, test_loss = train(\n",
    "            net, X_val_train, y_val_train, X_val_test, y_val_test,\n",
    "            epochs, verbose_epoch, learning_rate, weight_decay)\n",
    "        print(\"Train loss: %f\" % train_loss)\n",
    "        train_loss_sum += train_loss\n",
    "        print(\"Test loss: %f\" % test_loss)\n",
    "        test_loss_sum += test_loss\n",
    "    return train_loss_sum / k, test_loss_sum / k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-31T15:34:58.658620Z",
     "start_time": "2017-10-31T15:34:29.178581Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************\n",
      "learing_rate 3.3194233729175715\n",
      "hidden_nodes 34\n",
      "weight_decay 2.1647064517337613\n",
      "Epoch 96, train loss: 0.121311\n",
      "Epoch 97, train loss: 0.190961\n",
      "Epoch 98, train loss: 0.132296\n",
      "Epoch 99, train loss: 0.146928\n",
      "Train loss: 0.146928\n",
      "Test loss: 0.691250\n",
      "Epoch 96, train loss: 0.129613\n",
      "Epoch 97, train loss: 0.119664\n",
      "Epoch 98, train loss: 0.139758\n",
      "Epoch 99, train loss: 0.143571\n",
      "Train loss: 0.143571\n",
      "Test loss: 0.194584\n",
      "Epoch 96, train loss: 0.113584\n",
      "Epoch 97, train loss: 0.112705\n",
      "Epoch 98, train loss: 0.121567\n",
      "Epoch 99, train loss: 0.121465\n",
      "Train loss: 0.121465\n",
      "Test loss: 0.179014\n",
      "Epoch 96, train loss: 0.137322\n",
      "Epoch 97, train loss: 0.132800\n",
      "Epoch 98, train loss: 0.153251\n",
      "Epoch 99, train loss: 0.146187\n",
      "Train loss: 0.146187\n",
      "Test loss: 0.199072\n",
      "Epoch 96, train loss: 0.167234\n",
      "Epoch 97, train loss: 0.153421\n",
      "Epoch 98, train loss: 0.110716\n",
      "Epoch 99, train loss: 0.120875\n",
      "Train loss: 0.120875\n",
      "Test loss: 0.171005\n",
      "average_train_loss 0.286984908168\n",
      "average_test_loss 0.286984908168\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "best_learing_rate 3.3194233729175715\n",
      "best_hidden_nodes 34\n",
      "best_weight_decay 2.1647064517337613\n",
      "min_avg_test_loss 0.286984908168\n"
     ]
    }
   ],
   "source": [
    "learning_rate_range = 10\n",
    "weight_decay_range = 10\n",
    "hidden_min_nodes = 10\n",
    "hidden_max_nodes = 100\n",
    "\n",
    "for times in range(1):\n",
    "    best_learning_rate = None\n",
    "    best_hidden_nodes = None\n",
    "    best_weight_decay = None\n",
    "    first_time = True\n",
    "    min_avg_test_loss = None\n",
    "    learning_rate = random.uniform(0, learning_rate_range)\n",
    "    hidden_nodes = random.randint(hidden_min_nodes, hidden_max_nodes)\n",
    "    weight_decay = random.uniform(0, weight_decay_range)\n",
    "    net = get_net(hidden_nodes)\n",
    "    print('*'*100)\n",
    "    print('learing_rate', learning_rate)\n",
    "    print('hidden_nodes', hidden_nodes)\n",
    "    print('weight_decay', weight_decay)\n",
    "    average_train_loss, average_test_loss = k_fold_cross_valid(k, epochs, verbose_epoch, X_train, y_train, \n",
    "                                                               learning_rate, weight_decay, net)\n",
    "    print('average_train_loss', average_test_loss)\n",
    "    print('average_test_loss', average_test_loss)\n",
    "    if first_time:\n",
    "        min_avg_test_loss = average_test_loss\n",
    "        best_learning_rate = learning_rate\n",
    "        best_hidden_nodes = hidden_nodes\n",
    "        best_weight_decay = weight_decay\n",
    "        first_time = False\n",
    "    else:\n",
    "        if min_avg_test_loss > average_test_loss:\n",
    "            min_avg_test_loss = average_test_loss\n",
    "            best_learning_rate = learning_rate\n",
    "            best_hidden_nodes = hidden_nodes\n",
    "            best_weight_decay = weight_decay\n",
    "print('+'*100)\n",
    "print('best_learing_rate', best_learning_rate)\n",
    "print('best_hidden_nodes', best_hidden_nodes)\n",
    "print('best_weight_decay', best_weight_decay)\n",
    "print('min_avg_test_loss', min_avg_test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-31T15:45:34.520987Z",
     "start_time": "2017-10-31T15:45:34.507991Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def learn(epochs, verbose_epoch, X_train, y_train, test, learning_rate,\n",
    "          weight_decay, hidden_nodes):\n",
    "    net = get_net(hidden_nodes)\n",
    "    train(net, X_train, y_train, None, None, epochs, verbose_epoch,\n",
    "          learning_rate, weight_decay)\n",
    "    preds = net(X_test).asnumpy()\n",
    "    test['SalePrice'] = pd.Series(preds.reshape(1, -1)[0])\n",
    "    submission = pd.concat([test['Id'], test['SalePrice']], axis=1)\n",
    "    submission.to_csv('submission.csv', index=False)\n",
    "    param_series = pd.Series([learning_rate, weight_decay, hidden_nodes], index=['best_learning_rate', 'best_weight_decay', \n",
    "                                                                          'best_hidden_nodes'])\n",
    "    print(param_series)\n",
    "    param = param_series.to_frame().reset_index()\n",
    "    param = param.rename(columns= {0: 'value'})\n",
    "    param.index.name = 'param'\n",
    "    param.to_csv('param.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-31T15:45:44.149796Z",
     "start_time": "2017-10-31T15:45:36.995250Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96, train loss: 0.138537\n",
      "Epoch 97, train loss: 0.138276\n",
      "Epoch 98, train loss: 0.127814\n",
      "Epoch 99, train loss: 0.128603\n",
      "best_learning_rate     3.319423\n",
      "best_weight_decay      2.164706\n",
      "best_hidden_nodes     34.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "learn(epochs, verbose_epoch, X_train, y_train, test, best_learning_rate,\n",
    "      best_weight_decay, best_hidden_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}