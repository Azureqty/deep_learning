<!-- #region -->
《动手学深度学习》2.0.0-alpha0 版
========================


## 主要特点

该版本为《动手学深度学习》2.0.0-alpha0 版的预发布版本， 代码包含 MXNet, PyTorch 和 TensorFlow 三部分内容，供读者自由选择。

此外，同1.0版相比，本书2.0版绝大部分章节的内容（包括文字、数学、图片和代码）都有重新修订，并添加新的内容，详细改进见下。

本书 (2.0版) 的纸质版将由人民邮电出版社出版，预计在2021年夏季与读者见面。纸质版的索引、样式、排版、表述等均由出版社做了进一步的改进。


## 主要改进

* 将本书的网址由 zh.d2l.ai 改为了zh-v2.d2l.ai；
* 将本书的常用函数包的名称由d2lzh改为了 d2l ， 并分为 mxnet.py，torch.py 和 tensorflow.py 三个包；
* 添加 “符号” 章节，便于读者查阅本书常用符号。
* 丰富第2章“预备知识”内容，如：添加“数据预处理”，“线性代数”，“微分” 和 “概率”小结；
* 丰富1.0版第3章“深度学习基础”的内容，并分为“线性神经网络”和“多层感知机”两章；
* 丰富1.0版第5章的“卷积神经网络”的内容，并在2.0版分为“卷积神经网络”和“深度卷积神经网络”两章；
* 丰富1.0版第6章的“循环神经网络”的内容，并在2.0版分为“循环神经网络”和“循环卷积神经网络”两章(后者还在紧张编写中)；


## 详细改进

2.0.0-alpha0 版一共含50个小节，并分为以下篇章：

* 第1章： 深度学习前言
* 第2章： 预备知识

    * 2.1. 数据操作
        * 2.1.1. 入门
        * 2.1.2. 操作
        * 2.1.3. 广播机制
        * 2.1.4. 索引和切片
        * 2.1.5. 节省内存
        * 2.1.6. 转换为其他 Python 对象
        * 2.1.7. 小结
        * 2.1.8. 练习
    * 2.2. 数据预处理
        * 2.2.1. 读取数据集
        * 2.2.2. 处理缺失值
        * 2.2.3. 转换为张量格式
        * 2.2.4. 小结
        * 2.2.5. 练习
    * 2.3. 线性代数
        * 2.3.1. 标量
        * 2.3.2. 向量
        * 2.3.3. 矩阵
        * 2.3.4. 张量
        * 2.3.5. 张量算法的基本性质
        * 2.3.6. 汇总
        * 2.3.7. 点积（Dot Product）
        * 2.3.8. 矩阵-向量积
        * 2.3.9. 矩阵-矩阵乘法
        * 2.3.10. 范数
        * 2.3.11. 关于线性代数的更多信息
        * 2.3.12. 小结
        * 2.3.13. 练习
    * 2.4. 微分
        * 2.4.1. 导数和微分
        * 2.4.2. 偏导数
        * 2.4.3. 梯度
        * 2.4.4. 链式法则
        * 2.4.5. 小结
        * 2.4.6. 练习
    * 2.5. 自动求导
        * 2.5.1. 一个简单的例子
        * 2.5.2. 非标量变量的反向传播
        * 2.5.3. 分离计算
        * 2.5.4. Python控制流的梯度计算
        * 2.5.5. 小结
        * 2.5.6. 练习
    * 2.6. 概率
        * 2.6.1. 基本概率论
        * 2.6.2. 处理多个随机变量
        * 2.6.3. 期望和差异
        * 2.6.4. 小结
        * 2.6.5. 练习
    * 2.7. 查阅文档
        * 2.7.1. 查找模块中的所有函数和类
        * 2.7.2. 查找特定函数和类的用法
        * 2.7.3. 小结
        * 2.7.4. 练习

2.1.1. 入门
    * 2.1.2. 操作
    * 2.1.3. 广播机制
    * 2.1.4. 索引和切片
    * 2.1.5. 节省内存
    * 2.1.6. 转换为其他 Python 对象
    * 2.1.7. 小结
    * 2.1.8. 练习
* 2.2. 数据预处理
    * 2.2.1. 读取数据集
    * 2.2.2. 处理缺失值
    * 2.2.3. 转换为张量格式
    * 2.2.4. 小结
    * 2.2.5. 练习
* 2.3. 线性代数
    * 2.3.1. 标量
    * 2.3.2. 向量
    * 2.3.3. 矩阵
    * 2.3.4. 张量
    * 2.3.5. 张量算法的基本性质
    * 2.3.6. 汇总
    * 2.3.7. 点积（Dot Product）
    * 2.3.8. 矩阵-向量积
    * 2.3.9. 矩阵-矩阵乘法
    * 2.3.10. 范数
    * 2.3.11. 关于线性代数的更多信息
    * 2.3.12. 小结
    * 2.3.13. 练习
* 2.4. 微分
    * 2.4.1. 导数和微分
    * 2.4.2. 偏导数
    * 2.4.3. 梯度
    * 2.4.4. 链式法则
    * 2.4.5. 小结
    * 2.4.6. 练习
* 2.5. 自动求导
    * 2.5.1. 一个简单的例子
    * 2.5.2. 非标量变量的反向传播
    * 2.5.3. 分离计算
    * 2.5.4. Python控制流的梯度计算
    * 2.5.5. 小结
    * 2.5.6. 练习
* 2.6. 概率
    * 2.6.1. 基本概率论
    * 2.6.2. 处理多个随机变量
    * 2.6.3. 期望和差异
    * 2.6.4. 小结
    * 2.6.5. 练习
* 2.7. 查阅文档
    * 2.7.1. 查找模块中的所有函数和类
    * 2.7.2. 查找特定函数和类的用法
    * 2.7.3. 小结
    * 2.7.4. 练习

* 第3章：线性神经网络
    * 3.1. 线性回归
        * 3.1.1. 线性回归的基本元素
        * 3.1.2. 矢量化加速
        * 3.1.3. 正态分布与平方损失
        * 3.1.4. 从线性回归到深度网络
        * 3.1.5. 总结
        * 3.1.6. 练习
    * 3.2. 线性回归的从零开始实现
        * 3.2.1. 生成数据集
        * 3.2.2. 读取数据集
        * 3.2.3. 初始化模型参数
        * 3.2.4. 定义模型
        * 3.2.5. 定义损失函数
        * 3.2.6. 定义优化算法
        * 3.2.7. 训练
        * 3.2.8. 小结
        * 3.2.9. 练习
    * 3.3. 线性回归的简洁实现
        * 3.3.1. 生成数据集
        * 3.3.2. 读取数据集
        * 3.3.3. 定义模型
        * 3.3.4. 初始化模型参数
        * 3.3.5. 定义损失函数
        * 3.3.6. 定义优化算法
        * 3.3.7. 训练
        * 3.3.8. 总结
        * 3.3.9. 练习
    * 3.4. softmax回归
        * 3.4.1. 分类问题
        * 3.4.2. 网络结构
        * 3.4.3. 全连接层的参数开销
        * 3.4.4. softmax操作
        * 3.4.5. 小批量样本的矢量化
        * 3.4.6. 损失函数
        * 3.4.7. 信息论基础
        * 3.4.8. 模型预测和评估
        * 3.4.9. 小结
        * 3.4.10. 练习
    * 3.5. 图像分类数据集
        * 3.5.1. 读取数据集
        * 3.5.2. 读取小批量
        * 3.5.3. 整合所有组件
        * 3.5.4. 总结
        * 3.5.5. 练习
    * 3.6. softmax回归的从零开始实现
        * 3.6.1. 初始化模型参数
        * 3.6.2. 定义softmax操作
        * 3.6.3. 定义模型
        * 3.6.4. 定义损失函数
        * 3.6.5. 分类准确率
        * 3.6.6. 训练
        * 3.6.7. 预测
        * 3.6.8. 小结
        * 3.6.9. 练习
    * 3.7. softmax回归的简洁实现
        * 3.7.1. 初始化模型参数
        * 3.7.2. 重新审视Softmax的实现
        * 3.7.3. 优化算法
        * 3.7.4. 训练
        * 3.7.5. 总结
        * 3.7.6. 练习
* 第4章： 多层感知机
    * 4.1. 多层感知机
        * 4.1.1. 隐藏层
        * 4.1.2. 激活函数
        * 4.1.3. 小结
        * 4.1.4. 练习
    * 4.2. 多层感知机的从零开始实现
        * 4.2.1. 初始化模型参数
        * 4.2.2. 激活函数
        * 4.2.3. 模型
        * 4.2.4. 损失函数
        * 4.2.5. 训练
        * 4.2.6. 小结
        * 4.2.7. 练习
    * 4.3. 多层感知机的简洁实现
        * 4.3.1. 模型
        * 4.3.2. 小结
        * 4.3.3. 练习
    * 4.4. 模型选择、欠拟合和过拟合
        * 4.4.1. 训练误差和泛化误差
        * 4.4.2. 模型选择
        * 4.4.3. 欠拟合还是过拟合？
        * 4.4.4. 多项式回归
        * 4.4.5. 小结
        * 4.4.6. 练习
    * 4.5. 权重衰减
        * 4.5.1. 范数与权重衰减
        * 4.5.2. 高维线性回归
        * 4.5.3. 从零开始实现
        * 4.5.4. 简洁实现
        * 4.5.5. 小结
        * 4.5.6. 练习
    * 4.6. Dropout
        * 4.6.1. 重新审视过拟合
        * 4.6.2. 扰动的鲁棒性
        * 4.6.3. 实践中的dropout
        * 4.6.4. 从零开始实现
        * 4.6.5. 简洁实现
        * 4.6.6. 小结
        * 4.6.7. 练习
    * 4.7. 正向传播、反向传播和计算图
        * 4.7.1. 正向传播
        * 4.7.2. 正向传播计算图
        * 4.7.3. 反向传播
        * 4.7.4. 训练神经网络
        * 4.7.5. 小结
        * 4.7.6. 练习
    * 4.8. 数值稳定性和模型初始化
        * 4.8.1. 梯度消失和梯度爆炸
        * 4.8.2. 参数初始化
        * 4.8.3. 小结
        * 4.8.4. 练习
    * 4.9. 环境和分布偏移
        * 4.9.1. 分布偏移的类型
        * 4.9.2. 分布偏移示例
        * 4.9.3. 分布偏移纠正
        * 4.9.4. 学习问题的分类法
        * 4.9.5. 机器学习中的公平、责任和透明度
        * 4.9.6. 小结
        * 4.9.7. 练习
    * 4.10. 实战 Kaggle 比赛：预测房价
        * 4.10.1. 下载和缓存数据集
        * 4.10.2. Kaggle
        * 4.10.3. 访问和读取数据集
        * 4.10.4. 数据预处理
        * 4.10.5. 训练
        * 4.10.6. K 折交叉验证
        * 4.10.7. 模型选择
        * 4.10.8. 提交Kaggle的预测
        * 4.10.9. 小结
        * 4.10.10. 练习
* 第5章： 深度学习计算
    * 5.1. 层和块
        * 5.1.1. 自定义块
        * 5.1.2. 顺序块
        * 5.1.3. 在正向传播函数中执行代码
        * 5.1.4. 效率
        * 5.1.5. 小结
        * 5.1.6. 练习
    * 5.2. 参数管理
        * 5.2.1. 参数访问
        * 5.2.2. 参数初始化
        * 5.2.3. 参数绑定
        * 5.2.4. 小结
        * 5.2.5. 练习
    * 5.3. 延后初始化
        * 5.3.1. 实例化网络
        * 5.3.2. 小结
        * 5.3.3. 练习
    * 5.4. 自定义层
        * 5.4.1. 不带参数的层
        * 5.4.2. 带参数的图层
        * 5.4.3. 小结
        * 5.4.4. 练习
    * 5.5. 读写文件
        * 5.5.1. 加载和保存张量
        * 5.5.2. 加载和保存模型参数
        * 5.5.3. 小结
        * 5.5.4. 练习
    * 5.6. GPU
        * 5.6.1. 计算设备
        * 5.6.2. 张量与gpu
        * 5.6.3. 神经网络与GPU
        * 5.6.4. 小结
        * 5.6.5. 练习
* 第6章： 卷积神经网络
    * 6.1. 从全连接层到卷积
        * 6.1.1. 不变性
        * 6.1.2. 限制多层感知机
        * 6.1.3. 卷积
        * 6.1.4. “沃尔多在哪里” 回顾
        * 6.1.5. 小结
        * 6.1.6. 练习
    * 6.2. 图像卷积
        * 6.2.1. 互相关运算
        * 6.2.2. 卷积层
        * 6.2.3. 图像中目标的边缘检测
        * 6.2.4. 学习卷积核
        * 6.2.5. 互相关和卷积
        * 6.2.6. 特征映射和感受野
        * 6.2.7. 小结
        * 6.2.8. 练习
    * 6.3. 填充和步幅
        * 6.3.1. 填充
        * 6.3.2. 步幅
        * 6.3.3. 小结
        * 6.3.4. 练习
    * 6.4. 多输入多输出通道
        * 6.4.1. 多输入通道
        * 6.4.2. 多输出通道
        * 6.4.3. \(1\times 1\) 卷积层
        * 6.4.4. 小结
        * 6.4.5. 练习
    * 6.5. 池化层
        * 6.5.1. 最大池化层和平均池化层
        * 6.5.2. 填充和步幅
        * 6.5.3. 多个通道
        * 6.5.4. 小结
        * 6.5.5. 练习
    * 6.6. 卷积神经网络（LeNet）
        * 6.6.1. LeNet
        * 6.6.2. 模型训练
        * 6.6.3. 小结
        * 6.6.4. 练习
* 第7章： 深度卷积神经网络
    * 7.1. 深度卷积神经网络（AlexNet）
        * 7.1.1. 学习表征
        * 7.1.2. AlexNet
        * 7.1.3. 读取数据集
        * 7.1.4. 训练AlexNet
        * 7.1.5. 小结
        * 7.1.6. 练习
    * 7.2. 使用块的网络（VGG）
        * 7.2.1. VGG 块
        * 7.2.2. VGG 网络
        * 7.2.3. 训练
        * 7.2.4. 小结
        * 7.2.5. 练习
    * 7.3. 网络中的网络（NiN）
        * 7.3.1. NiN 块
        * 7.3.2. NiN 模型
        * 7.3.3. 训练
        * 7.3.4. 小结
        * 7.3.5. 练习
    * 7.4. 含并行连结的网络（GoogLeNet）
        * 7.4.1. Inception块
        * 7.4.2. GoogLeNet 模型
        * 7.4.3. 训练
        * 7.4.4. 小结
        * 7.4.5. 练习
    * 7.5. 批量归一化
        * 7.5.1. 训练深层网络
        * 7.5.2. 批量归一化层
        * 7.5.3. 从零实现
        * 7.5.4. 使用批量归一化层的 LeNet
        * 7.5.5. 简明实现
        * 7.5.6. 争议
        * 7.5.7. 小结
        * 7.5.8. 练习
    * 7.6. 残差网络（ResNet）
        * 7.6.1. 函数类
        * 7.6.2. 残差块
        * 7.6.3. ResNet模型
        * 7.6.4. 训练 ResNet
        * 7.6.5. 小结
        * 7.6.6. 练习
    * 7.7. 稠密连接网络（DenseNet）
        * 7.7.1. 从ResNet到DenseNet
        * 7.7.2. 稠密块体
        * 7.7.3. 过渡层
        * 7.7.4. DenseNet模型
        * 7.7.5. 训练模型
        * 7.7.6. 小结
        * 7.7.7. 练习
* 第8章： 循环神经网络
    * 8.1. 序列模型
        * 8.1.1. 统计工具
        * 8.1.2. 训练
        * 8.1.3. 预测
        * 8.1.4. 小结
        * 8.1.5. 练习
    * 8.2. 文本预处理
        * 8.2.1. 读取数据集
        * 8.2.2. 标记化
        * 8.2.3. 词汇
        * 8.2.4. 把所有的东西放在一起
        * 8.2.5. 小结
        * 8.2.6. 练习
    * 8.3. 语言模型和数据集
        * 8.3.1. 学习语言模型
        * 8.3.2. 马尔可夫模型与 𝑛 元语法
        * 8.3.3. 自然语言统计
        * 8.3.4. 读取长序列数据
        * 8.3.5. 小结
        * 8.3.6. 练习
    * 8.4. 循环神经网络
        * 8.4.1. 无隐藏状态的神经网络
        * 8.4.2. 具有隐藏状态的循环神经网络
        * 8.4.3. 基于循环神经网络的字符级语言模型
        * 8.4.4. 困惑度（Perplexity）
        * 8.4.5. 小结
        * 8.4.6. 练习
    * 8.5. 循环神经网络的从零开始实现
        * 8.5.1. 独热编码
        * 8.5.2. 初始化模型参数
        * 8.5.3. 循环神经网络模型
        * 8.5.4. 预测
        * 8.5.5. 梯度裁剪
        * 8.5.6. 训练
        * 8.5.7. 小结
        * 8.5.8. 练习
    * 8.6. 循环神经网络的简洁实现
        * 8.6.1. 定义模型
        * 8.6.2. 训练与预测
        * 8.6.3. 小结
        * 8.6.4. 练习
    * 8.7. 通过时间反向传播
        * 8.7.1. 循环神经网络的梯度分析
        * 8.7.2. 通过时间反向传播细节
        * 8.7.3. 小结
        * 8.7.4. 练习



英文版0.16版已发布：https://github.com/d2l-ai/d2l-en/releases/tag/v0.16.0， 欢迎关注[本书第二版的英文开源项目](https://github.com/d2l-ai/d2l-en/)。我们将同时不断更新中文版，翻译其他章节。

我们的初衷是让更多人更容易地使用深度学习。为了让大家能够便利地获取这些资源，我们一如既往保留了免费的网站内容，并且通过不收取出版稿费的方式来降低纸质书的价格，使更多人有能力购买。

<!-- #endregion -->
